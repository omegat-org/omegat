# Machine Translation SPI with multi-level context for LLM Support

## Status
**Proposed**

## Target Development Cycle
6.2.0_Beta

## Date
2025-12-27

## Context
OmegaT users have expressed interest in using Large Language Models (LLMs) for machine translation, specifically local LLMs to ensure data privacy and avoid recurring costs.
Frameworks like Ollama, LM Studio, llama.cpp, and GPT4All provide OpenAI-compatible APIs that can be leveraged on a user's local machine.

The existing Machine Translation (MT) Service Provider Interface (SPI) in OmegaT is primarily designed for simple segment-by-segment translation without persistent context.
While local machine translation is already possible in OmegaT through plugins like Apertium, Large Language Models (LLMs) introduce new requirements, such as the need for persistent context to produce high-quality, consistent translations.
Improving this SPI to support project-level context not only enables local LLM integration but also paves the way for advanced remote LLM connectors (e.g., OpenAI, Azure OpenAI, Google Gemini, Anthropic Claude, Mistral, Lumo AI, and GitHub Copilot) to provide higher-quality, context-aware translations.

Furthermore, a "2-level prompting" approach has been proposed to improve translation quality:
1.  **Project-level context**: A persistent prompt containing domain information, style guides, target audience, etc., applicable to the entire project.
2.  **Segment-level prompting**: Sending the source segment along with the project-level context and a specific instruction (e.g., "translate this segment").

This approach allows for more tailored translations compared to simple segment-by-segment requests.
To make this approach practical, we will enhance the OmegaT MT SPI to handle project-level context and introduce a new **external, optional plugin** that provides an LLM-powered MT connector and translation quality checking features.

## Decision
We will implement support for multi-level context SPI in OmegaT.

### 1. Enhanced MachineTranslation SPI for external optional plugin

The `IMachineTranslation` interface in OmegaT core will be extended to support contexts. 
The actual LLM support implementation will be provided by an optional plugin that will be developed and maintained as a **separate project**, independent of the OmegaT.

### 2. Future-Proofing the SPI for a Translation Quality Checker (Reasoning Feature)

The initial implementation focused on the existing full-segment translation model.
However, we acknowledge that LLMs are capable of more, such as explaining their reasoning or providing document-level alignments. 
We will design SPI enhancements with more granular "suggestions" and metadata in mind, allowing for better alignment and explanation features to be passed from the engine to the OmegaT editor.

### Proposed changes

- **`org.omegat.core.data.IContextSupplier`**: New interface for providing project-level context to MT connectors and Issue Providers.
- **`org.omegat.gui.exttrans.IMachineTranslation`**: Updated SPI with a setter/getter for project context and model selection.
- **`org.omegat.gui.exttrans.IMTGlossarySupplier`**: Existing SPI leveraged to provide terminology context for better word-level translation accuracy.
- **`org.omegat.gui.issues.IIssueProvider`**: Updated SPI to include reasoning and alignment features.

## Consequences

### Positive
✅ **Clean Core**: Keeping LLM implementation external prevents the core from becoming bloated with rapidly changing LLM-specific logic and dependencies.
✅ **Independent Release Cycle**: The LLM plugin can be updated and released independently of OmegaT core, allowing for faster response to changes in LLM APIs.

### Negative
- **Complexity**: Increases the surface area of project settings and adds a new layer of plugin-managed configuration.

### Neutral


## Optional Note (out of the scope of this proposal)

### External LLM plugin design

#### 1. MT connectors to support OpenAI-compatible APIs and others
External LLM plugin implements:
- **`OpenAICompatibleLLM`**: A new generic connector for OpenAI-compatible Chat Completion APIs provided by the plugin.
  This supports:
  - **Local LLMs**: Running via Ollama, LM Studio, etc., on the user's machine.
  - **Private LLMs**: Running on private clouds or servers with OpenAI compatible API endpoints.
  - **Public LLMs**: Public service with OpenAI API.
- **Dedicated Connectors**: Individual MT connectors can be developed for services;
  - **Public services**: Azure, Google, Lumo AI, Anthropic, Mistral, and Copilot, each leveraging the new context-aware SPI.
  - **Private LLMs**: Running on private clouds or servers with custom API endpoints.

#### 2. Project-Level Configuration and Storage
We will introduce a project-specific configuration file to store LLM-related settings, including the model selection and project-level context.
- **`omegat/llm.xml`**: This new file will store LLM configurations.
  By using a separate file instead of extending the core `omegat.project` (internal `ProjectProperties`), we ensure **backward compatibility** and **stability** for older OmegaT versions that do not support these features.
- **Project-Level Selection**: Users can select and configure different LLM models (local or remote) on a per-project basis.
- **Project-Level Context**: A new field for persistent prompt context (domain, style, etc.).

#### 3. Team Project Support
To facilitate collaboration in team projects:
- **Shared Configuration**: The `omegat/llm.xml` file will be included in the team repository.
  This allows the entire team to share a common project-level context and pre-configured LLM models.
- **Consistency**: Ensures all team members use the same contextual instructions for machine translation, leading to more consistent results across the project.

#### 4. Prompt Construction Logic
The LLM connectors will construct the final prompt by combining:
- The **System-Level Context** (sent as a `system` message) with the connector prompt context (e.g., "You are a helpful assistant for machine translation. You will translate segments of text from one language to another. Your instructions start with the phrase:").
- The **Project-Level Context** (sent as a `project` message) with the user-entered project prompt context (e.g., "Translate English to French in a formal tone for mathematical formulas and technical documents.").
- The **Terminology Context** (sent as a `project` or `user` message): Leverages the existing `IMTGlossarySupplier` SPI to provide relevant glossary terms for the segment. This allows the LLM to adhere to project-specific terminology, improving translation consistency and accuracy.
- The **Segment-Level Prompt** (sent as a `user` message), which includes the source text and standard instructions like "Translate the following text from {srcLang} to {targetLang}".

#### 5. LLM Issues Detection and Reasoning
The plugin can be used to evaluate translation quality (e.g., checking for grammar, style, or meaning preservation).
This will be implemented in the LLM plugin by extending the `org.omegat.gui.issues` package.
- **`IIssueProvider`**: The plugin will implement new issue providers that leverage LLM services (local or remote) to identify potential translation problems.
- **Reasoning**: LLMs can provide detailed explanations (reasoning) for why a certain segment might have issues, which can be displayed in the Issues window.

#### 6. Multi-Feature Plugin Implementation
The optional LLM plugin will provide both MT connector and Issue Provider features and will be developed and maintained as a **separate project in its own repository**, independent of the OmegaT core repository. To avoid duplication:
- **Plugin Manifest**: The plugin can use a complex manifest structure with multiple sections to define different plugin categories (`machinetranslator` and `miscellaneous` or similar).
- **Centralize Common Logic**: Shared logic for communicating with LLM APIs (OpenAI-compatible) and managing prompt construction will be implemented within the plugin (e.g., in `org.omegat.plugin.llm`).
- **Shared Configuration**: Both components within the plugin will use a shared internal configuration model (`LLMProperties`) backed by `omegat/llm.xml`.
- **Externalization**: The entire `org.omegat.plugin.llm` package and its related resources will live in a separate project and repository, ensuring that OmegaT core remains decoupled from specific LLM implementations.
- **Mitigating Duplication**: By providing a unified internal API for LLM interactions within the plugin, we reduce the risk of code duplication and simplify maintenance.

#### Technical Architecture

```
┌──────────────────────────────────────┐
│   Optional LLM Plugin (JAR)          │
│                                      │
│  ┌────────────────────────────────┐  │
│  │   UI: LLM Configuration        │  │
│  └──────────────┬─────────────────┘  │
│                 │                    │
│                 ↓                    │
│  ┌────────────────────────────────┐  │
│  │   LLMProperties                │──┼──→ (Storage: omegat/llm.xml)
│  └──────────────┬─────────────────┘  │
│                 │                    │
│        ┌────────┴─────────┐          │
│        ↓                  ↓          │
│  ┌────────────┐    ┌────────────┐    │
│  │ MTConnector│    │ Issue Prov.│    │
│  │ (MT SPI)   │    │ (Issue SPI)│    │
│  └────────────┘    └────────────┘    │
│        │                  │          │
│        └────────┬─────────┘          │
│                 ↓                    │
│  ┌────────────────────────────────┐  │
│  │   Shared Plugin Infrastructure │  │
│  │   (API & Prompt Construction)  │  │
│  └──────────────┬─────────────────┘  │
└─────────────────┼────────────────────┘
                  │
                  ↓
┌──────────────────────────────────────────────────────────────────────────┐
│   LLM API (Local or Remote: Ollama, OpenAI, Claude, Azure, etc.)         │
└──────────────────────────────────────────────────────────────────────────┘
```

#### Proposed Additions
- **`org.omegat.plugin.llm.LLMProperties`**: Plugin-internal class for model selection and `projectPromptContext` fields, with XML serialization to `omegat/llm.xml`.
- **`org.omegat.plugin.llm.LLMPropertiesDialog`**: UI components for selecting LLM models and editing the project prompt context.
- **`org.omegat.plugin.llm.LLMIssueProvider`**: New implementation of `IIssueProvider` for translation quality checks.
- **`org.omegat.plugin.llm.internal`**: Shared utilities for API communication and prompt construction logic.
- **`org.omegat.plugin.llm.OpenAICompatibleLLM`**: New generic LLM connector implementing `IMachineTranslation`.
- **`org.omegat.plugin.llm.PluginEntry`**: Initialization class with `loadPlugins()` to register components and `CoreEvents` listeners.

## Consequences

### Positive
✅ **Privacy**: Users can use local or private models without sending data to third-party public cloud services.
✅ **Cost-Effective**: No per-token costs for local or privately hosted execution.
✅ **Extensibility**: Users can use local, private, and public models interchangeably.
✅ **Quality**: multi-level prompting provides LLMs with better context for more accurate and stylistically consistent translations.
✅ **Flexibility**: Support for OpenAI-compatible APIs allows users to switch between many different LLM backends and models easily.

### Neutral
-   **Performance**: Local LLM performance depends on the user's hardware (GPU/RAM).
-   **Setup**: Users need to install and configure an external LLM framework (e.g., Ollama).

### Negative
-   **Maintenance**: LLM APIs and prompt engineering best practices evolve quickly, requiring frequent plugin updates.
-   **Plugin Coordination**: Providing multiple features (MT and Issues) within a single plugin requires careful design of shared internal components to ensure consistency and prevent resource leaks.

## References
- [OpenAI-compatible APIs](https://platform.openai.com/docs/guides/openai-compatible-apis)
